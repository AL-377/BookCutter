{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your file to cut\n",
    "pdf_path = 'books/Principles_of_Microeconomics.txt'\n",
    "# chosen model, make sure you have downloaded it using ollama\n",
    "DEFAULT_MODEL = \"llama3.2:1b\"\n",
    "##########\n",
    "# 1. Pre-Process PDF to Text\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from typing import Optional\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary: Call LLM Model using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = \"\"\"\n",
    "You are a world class text pre-processor, here is the raw data from a PDF, please parse and return it in a way that is crispy and usable to send to a podcast writer.\n",
    "\n",
    "The raw data is messed up with new lines, Latex math and you will see fluff that we can remove completely. Basically take away any details that you think might be useless in a podcast author's transcript.\n",
    "\n",
    "Remember, the podcast could be on any topic whatsoever so the issues listed above are not exhaustive\n",
    "\n",
    "Please be smart with what you remove and be creative ok?\n",
    "\n",
    "Remember DO NOT START SUMMARIZING THIS, YOU ARE ONLY CLEANING UP THE TEXT AND RE-WRITING WHEN NEEDED\n",
    "\n",
    "Be very smart and aggressive with removing details, you will get a running portion of the text and keep returning the processed text.\n",
    "\n",
    "PLEASE DO NOT ADD MARKDOWN FORMATTING, STOP ADDING SPECIAL CHARACTERS THAT MARKDOWN CAPATILISATION ETC LIKES\n",
    "\n",
    "ALWAYS start your response directly with processed text and NO ACKNOWLEDGEMENTS about my questions ok?\n",
    "Here is the text:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(model,messages,**kwargs):\n",
    "    \"\"\"Call Ollama API,before running this function, make sure you have the Ollama running on your local machine\"\"\"\n",
    "    import requests\n",
    "    import json\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"options\": {\n",
    "            \"seed\": kwargs.get(\"seed\", None),\n",
    "            \"temperature\": kwargs.get(\"temperature\", 0),\n",
    "        },\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    # print(response.json())\n",
    "    return response.json()[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pre-Process PDF to Text\n",
    "\n",
    "> refer to [NotebookLlama](https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/NotebookLlama/Step-1%20PDF-Pre-Processing-Logic.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_pdf(file_path: str) -> bool:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return False\n",
    "    if not file_path.lower().endswith('.pdf'):\n",
    "        print(\"Error: File is not a PDF\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_text_from_pdf(file_path: str, max_chars: int = 100000) -> Optional[str]:\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Create PDF reader object\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            # Get total number of pages\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"Processing PDF with {num_pages} pages...\")\n",
    "            \n",
    "            extracted_text = []\n",
    "            total_chars = 0\n",
    "            \n",
    "            # Iterate through all pages\n",
    "            for page_num in range(num_pages):\n",
    "                # Extract text from page\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                \n",
    "                # Check if adding this page's text would exceed the limit\n",
    "                if total_chars + len(text) > max_chars:\n",
    "                    # Only add text up to the limit\n",
    "                    remaining_chars = max_chars - total_chars\n",
    "                    extracted_text.append(text[:remaining_chars])\n",
    "                    print(f\"Reached {max_chars} character limit at page {page_num + 1}\")\n",
    "                    break\n",
    "                \n",
    "                extracted_text.append(text)\n",
    "                total_chars += len(text)\n",
    "                print(f\"Processed page {page_num + 1}/{num_pages}\")\n",
    "            \n",
    "            final_text = '\\n'.join(extracted_text)\n",
    "            print(f\"\\nExtraction complete! Total characters: {len(final_text)}\")\n",
    "            return final_text\n",
    "            \n",
    "    except PyPDF2.PdfReadError:\n",
    "        print(\"Error: Invalid or corrupted PDF file\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n",
    "# Get PDF metadata\n",
    "def get_pdf_metadata(file_path: str) -> Optional[dict]:\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            metadata = {\n",
    "                'num_pages': len(pdf_reader.pages),\n",
    "                'metadata': pdf_reader.metadata\n",
    "            }\n",
    "            return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata first\n",
    "print(\"Extracting metadata...\")\n",
    "metadata = get_pdf_metadata(pdf_path)\n",
    "if metadata:\n",
    "    print(\"\\nPDF Metadata:\")\n",
    "    print(f\"Number of pages: {metadata['num_pages']}\")\n",
    "    print(\"Document info:\")\n",
    "    for key, value in metadata['metadata'].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Extract text\n",
    "print(\"\\nExtracting text...\")\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Display first 500 characters of extracted text as preview\n",
    "if extracted_text:\n",
    "    print(\"\\nPreview of extracted text (first 500 characters):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(extracted_text[:500])\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"\\nTotal characters extracted: {len(extracted_text)}\")\n",
    "\n",
    "# Optional: Save the extracted text to a file\n",
    "if extracted_text:\n",
    "    output_file = 'extracted_text.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(extracted_text)\n",
    "    print(f\"\\nExtracted text has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load txt file and clean by LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_file(file_path: str,encoding='utf-8') -> Optional[str]:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return None\n",
    "    if not file_path.lower().endswith('.txt'):\n",
    "        print(\"Error: File is not a text file\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            text = file.read()\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = load_txt_file('/Users/al/Learning/LLM/repo/PDF2Podcast/micro_economic.txt', encoding='gb2312')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_bounded_chunks(text, target_chunk_size):\n",
    "    \"\"\"\n",
    "    Split text into chunks at word boundaries close to the target chunk size.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        word_length = len(word) + 1  # +1 for the space\n",
    "        if current_length + word_length > target_chunk_size and current_chunk:\n",
    "            # Join the current chunk and add it to chunks\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = word_length\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_length += word_length\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_chunk(text_chunk, chunk_num):\n",
    "    \"\"\"Process a chunk of text and return both input and output for verification\"\"\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": text_chunk},\n",
    "    ]\n",
    "\n",
    "    processed_text = llm_call(DEFAULT_MODEL,conversation,temperature=0.5)\n",
    "    \n",
    "   \n",
    "    # Print chunk information for monitoring\n",
    "    #print(f\"\\n{'='*40} Chunk {chunk_num} {'='*40}\")\n",
    "    print(f\"INPUT TEXT:\\n{text_chunk[:500]}...\")  # Show first 500 chars of input\n",
    "    print(f\"\\nPROCESSED TEXT:\\n{processed_text[:500]}...\")  # Show first 500 chars of output\n",
    "    print(f\"{'='*90}\\n\")\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"extracted_text.txt\"  # Replace with your file path\n",
    "CHUNK_SIZE = 1000  # Adjust chunk size if needed\n",
    "\n",
    "chunks = create_word_bounded_chunks(extracted_text, CHUNK_SIZE)\n",
    "num_chunks = len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f\"clean_{os.path.basename(INPUT_FILE)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = \"\"\n",
    "with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "\n",
    "    for chunk_num, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
    "        # Process chunk and append to complete text\n",
    "        processed_chunk = process_chunk(chunk, chunk_num)\n",
    "        processed_text += processed_chunk + \"\\n\"\n",
    "        # Write chunk immediately to file\n",
    "        out_file.write(processed_chunk + \"\\n\")\n",
    "        out_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cut the Txt file to sections by LLM\n",
    "\n",
    "- (1) Use regex to split the txt files\n",
    "- (2) Use LLM to judge whether the split is reasonable\n",
    "\n",
    "> Notice:  Should modify logic for different regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def llm_call(model, messages, **kwargs):\n",
    "    \"\"\"Call Ollama API, ensure Ollama is running on your local machine\"\"\"\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"options\": {\n",
    "            \"seed\": kwargs.get(\"seed\", None),\n",
    "            \"temperature\": kwargs.get(\"temperature\", 0),\n",
    "        },\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    return response.json()[\"message\"][\"content\"]\n",
    "\n",
    "def is_new_chapter(title, context):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个帮助识别章节分割的助手。\"},\n",
    "        {\"role\": \"user\", \"content\": f\"以下是一个章节标题和上下文，请判断是否应该在此处分割章节。\\n\\n章节标题: {title}\\n上下文: {context}\\n\\n请回答 '是' 或 '否'。\"}\n",
    "    ]\n",
    "    response = llm_call(\"llama3.2:1b\", messages, temperature=0)\n",
    "    return response.strip().lower() == '是'\n",
    "\n",
    "def is_directory_file(file_content):\n",
    "    \"\"\"Use GPT to determine if the file is a directory file\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个帮助识别目录文件的助手。\"},\n",
    "        {\"role\": \"user\", \"content\": f\"以下是文件内容的一部分，请判断是否该文件是不是属于一个目录:\\n\\n{file_content}\\n\\n回答 '是' 或 '否'。\"}\n",
    "    ]\n",
    "    response = llm_call(\"llama3.2:1b\", messages, temperature=0)\n",
    "    return response.strip().lower() == '是'\n",
    "\n",
    "\n",
    "def extract_chapter_titles(file_content):\n",
    "    \"\"\"Extract all chapter titles from the file\"\"\"\n",
    "    chapter_pattern = re.compile(r'(第[一二三四五六七八九十]+章\\s+.*)')\n",
    "    return chapter_pattern.findall(file_content)\n",
    "\n",
    "def find_matching_file(chapter_title, files_dir, toc_file):\n",
    "    \"\"\"Find the corresponding chapter file based on the chapter title\"\"\"\n",
    "    sanitized_title = re.sub(r'[\\\\/*?:\"<>|]', \"\", chapter_title)\n",
    "    for filename in os.listdir(files_dir):\n",
    "        if filename == toc_file:\n",
    "            continue\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(files_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                first_line = f.readline().strip()\n",
    "                sanitized_line = re.sub(r'[\\\\/*?:\"<>|]', \"\", first_line)\n",
    "                if sanitized_line in sanitized_title:\n",
    "                    return os.path.join(files_dir, filename)\n",
    "    return None\n",
    "\n",
    "def split_chapters(file_path):\n",
    "    with open(file_path, 'r', encoding='gbk') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    chapter_pattern = re.compile(r'^第[一二三四五六七八九十]+章\\s+.*')\n",
    "    chapters = []\n",
    "    current_chapter = []\n",
    "    chapter_title = \"\"\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if chapter_pattern.match(line):\n",
    "            context = ''.join(current_chapter[-5:])  # 上下文取最近5行\n",
    "            if is_new_chapter(line.strip(), context):\n",
    "                if current_chapter:\n",
    "                    chapters.append((chapter_title, ''.join(current_chapter)))\n",
    "                chapter_title = line.strip()\n",
    "                current_chapter = []\n",
    "            current_chapter.append(line)\n",
    "        else:\n",
    "            current_chapter.append(line)\n",
    "    \n",
    "    if current_chapter:\n",
    "        chapters.append((chapter_title, ''.join(current_chapter)))\n",
    "\n",
    "    output_dir = 'chapters'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for idx, (title, content) in enumerate(chapters, 1):\n",
    "        sanitized_title = re.sub(r'[\\\\/*?:\"<>|]', \"\", title[:30])\n",
    "        file_name = f\"Chapter{idx}_{sanitized_title}.txt\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "\n",
    "    print(f'All chapters splitted saved under \"{output_dir}\"')\n",
    "\n",
    "\n",
    "def merge_tocs_and_chapters(files_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Traverse all files in files_dir, identify directory files, match and merge corresponding chapter files, and save to output_dir\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(files_dir):\n",
    "        file_path = os.path.join(files_dir, filename)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "        \n",
    "        file = open(file_path, 'r', encoding='utf-8')\n",
    "        tocs = file.readlines()\n",
    "        preview = ''.join(tocs[:5])\n",
    "        if is_directory_file(preview):\n",
    "            print(f\"Toc file detected: {filename}\")\n",
    "            chapter_titles = extract_chapter_titles(preview)\n",
    "            merged_content = ''.join(tocs)\n",
    "            chapter_file = None\n",
    "            for title in chapter_titles:\n",
    "                chapter_file = find_matching_file(title, files_dir,os.path.basename(file_path))\n",
    "                if chapter_file:\n",
    "                    with open(chapter_file, 'r', encoding='utf-8') as cf:\n",
    "                        merged_content += f\"\\n\\n{cf.read()}\"\n",
    "                else:\n",
    "                    print(f\"Toc No detected: {title}\")\n",
    "            if chapter_file:\n",
    "                sanitized_title = re.sub(r'[\\\\/*?:\"<>|]', \"\", os.path.basename(chapter_file)[:30])\n",
    "                merged_filename = f\"Merged_{sanitized_title}\"\n",
    "                merged_file_path = os.path.join(output_dir, merged_filename)\n",
    "                with open(merged_file_path, 'w', encoding='utf-8') as mf:\n",
    "                    mf.write(merged_content)\n",
    "                print(f\"Merged file saved into: {merged_filename}\")\n",
    "        file.close()\n",
    "    print(f\"All files merged, saved under '{output_dir}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_chapters('micro_economic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tocs_and_chapters('chapters', 'merged_chapters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
